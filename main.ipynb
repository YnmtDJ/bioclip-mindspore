{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d94fbdf",
   "metadata": {},
   "source": [
    "# CLIP模型、Tokenizer定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5396c557",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     58\u001b[39m     text = text.strip()\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mSimpleTokenizer\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbpe_path\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbyte_encoder\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbytes_to_unicode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mSimpleTokenizer\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSimpleTokenizer\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, bpe_path: \u001b[38;5;28mstr\u001b[39m = \u001b[43mdefault_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m     64\u001b[39m         \u001b[38;5;28mself\u001b[39m.byte_encoder = bytes_to_unicode()\n\u001b[32m     65\u001b[39m         \u001b[38;5;28mself\u001b[39m.byte_decoder = {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.byte_encoder.items()}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mdefault_bpe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;129m@lru_cache\u001b[39m()\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_bpe\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m os.path.join(os.path.dirname(os.path.abspath(\u001b[34;43m__file__\u001b[39;49m)), \u001b[33m\"\u001b[39m\u001b[33mbpe_simple_vocab_16e6.txt.gz\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import html\n",
    "from functools import lru_cache\n",
    "\n",
    "import ftfy\n",
    "import regex as re\n",
    "import os\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def default_bpe():\n",
    "    return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"bpe_simple_vocab_16e6.txt.gz\")\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = html.unescape(html.unescape(text))\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def whitespace_clean(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "class SimpleTokenizer(object):\n",
    "    def __init__(self, bpe_path: str = default_bpe()):\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split(\"\\n\")\n",
    "        merges = merges[1 : 49152 - 256 - 2 + 1]\n",
    "        merges = [tuple(merge.split()) for merge in merges]\n",
    "        vocab = list(bytes_to_unicode().values())\n",
    "        vocab = vocab + [v + \"</w>\" for v in vocab]\n",
    "        for merge in merges:\n",
    "            vocab.append(\"\".join(merge))\n",
    "        vocab.extend([\"<|startoftext|>\", \"<|endoftext|>\"])\n",
    "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "        self.cache = {\"<|startoftext|>\": \"<|startoftext|>\", \"<|endoftext|>\": \"<|endoftext|>\"}\n",
    "        self.pat = re.compile(\n",
    "            r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\",\n",
    "            re.IGNORECASE,\n",
    "        )\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token[:-1]) + (token[-1] + \"</w>\",)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token + \"</w>\"\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except Exception:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                    new_word.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = \" \".join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        text = whitespace_clean(basic_clean(text)).lower()\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = \"\".join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=\"replace\").replace(\"</w>\", \" \")\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd299efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mindspore as ms\n",
    "from mindspore import Parameter, Tensor, load_param_into_net, nn, ops\n",
    "from mindspore.ops.function.nn_func import multi_head_attention_forward\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Cell):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            inplanes, planes, 1, has_bias=False, pad_mode=\"pad\", weight_init=\"uniform\", bias_init=\"uniform\"\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, 3, padding=1, has_bias=False, pad_mode=\"pad\", weight_init=\"uniform\", bias_init=\"uniform\"\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=stride, stride=stride, pad_mode=\"pad\") if stride > 1 else nn.Identity()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            planes,\n",
    "            planes * self.expansion,\n",
    "            1,\n",
    "            has_bias=False,\n",
    "            pad_mode=\"pad\",\n",
    "            weight_init=\"uniform\",\n",
    "            bias_init=\"uniform\",\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.downsample = None\n",
    "        self.stride = stride\n",
    "\n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
    "            self.downsample = nn.SequentialCell(\n",
    "                OrderedDict(\n",
    "                    [\n",
    "                        (\"9999\", nn.AvgPool2d(kernel_size=stride, stride=stride, pad_mode=\"pad\")),\n",
    "                        (\n",
    "                            \"0\",\n",
    "                            nn.Conv2d(\n",
    "                                inplanes,\n",
    "                                planes * self.expansion,\n",
    "                                1,\n",
    "                                stride=1,\n",
    "                                has_bias=False,\n",
    "                                pad_mode=\"pad\",\n",
    "                                weight_init=\"uniform\",\n",
    "                                bias_init=\"uniform\",\n",
    "                            ),\n",
    "                        ),\n",
    "                        (\"1\", nn.BatchNorm2d(planes * self.expansion)),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.relu2(self.bn2(self.conv2(out)))\n",
    "        out = self.avgpool(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionPool2d(nn.Cell):\n",
    "    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = Parameter(\n",
    "            ops.randn(spacial_dim**2 + 1, embed_dim, dtype=ms.float32) / embed_dim**0.5\n",
    "        )\n",
    "        self.k_proj = nn.Dense(embed_dim, embed_dim)\n",
    "        self.q_proj = nn.Dense(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Dense(embed_dim, embed_dim)\n",
    "        self.c_proj = nn.Dense(embed_dim, output_dim or embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = ops.flatten(x, start_dim=2).permute((2, 0, 1))  # NCHW -> (HW)NC\n",
    "        x = ops.cat([x.mean(axis=0, keep_dims=True), x], axis=0)  # (HW+1)NC\n",
    "        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n",
    "        x, _ = multi_head_attention_forward(\n",
    "            query=x[:1],\n",
    "            key=x,\n",
    "            value=x,\n",
    "            embed_dim_to_check=x.shape[-1],\n",
    "            num_heads=self.num_heads,\n",
    "            q_proj_weight=self.q_proj.weight,\n",
    "            k_proj_weight=self.k_proj.weight,\n",
    "            v_proj_weight=self.v_proj.weight,\n",
    "            in_proj_weight=None,\n",
    "            in_proj_bias=ops.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n",
    "            bias_k=None,\n",
    "            bias_v=None,\n",
    "            add_zero_attn=False,\n",
    "            dropout_p=0.0,\n",
    "            out_proj_weight=self.c_proj.weight,\n",
    "            out_proj_bias=self.c_proj.bias,\n",
    "            use_separate_proj_weight=True,\n",
    "            training=self.training,\n",
    "        )\n",
    "        return ops.squeeze(x, 0)\n",
    "\n",
    "\n",
    "class ModifiedResNet(nn.Cell):\n",
    "    \"\"\"\n",
    "    A ResNet class that contains the following changes:\n",
    "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "    - The final pooling layer is a QKV attention instead of an average pool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        # the 3-layer stem\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            3,\n",
    "            width // 2,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            has_bias=False,\n",
    "            pad_mode=\"pad\",\n",
    "            weight_init=\"uniform\",\n",
    "            bias_init=\"uniform\",\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            width // 2,\n",
    "            width // 2,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            has_bias=False,\n",
    "            pad_mode=\"pad\",\n",
    "            weight_init=\"uniform\",\n",
    "            bias_init=\"uniform\",\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            width // 2,\n",
    "            width,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            has_bias=False,\n",
    "            pad_mode=\"pad\",\n",
    "            weight_init=\"uniform\",\n",
    "            bias_init=\"uniform\",\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2, pad_mode=\"pad\", stride=2)\n",
    "\n",
    "        # residual layers\n",
    "        self._inplanes = width  # this is a *mutable* variable used during construction\n",
    "        self.layer1 = self._make_layer(width, layers[0])\n",
    "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
    "\n",
    "        embed_dim = width * 32  # the ResNet feature dimension\n",
    "        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "\n",
    "        self._inplanes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self._inplanes, planes))\n",
    "\n",
    "        return nn.SequentialCell(*layers)\n",
    "\n",
    "    def construct(self, x):\n",
    "        def stem(x):\n",
    "            x = self.relu1(self.bn1(self.conv1(x)))\n",
    "            x = self.relu2(self.bn2(self.conv2(x)))\n",
    "            x = self.relu3(self.bn3(self.conv3(x)))\n",
    "            x = self.avgpool(x)\n",
    "            return x\n",
    "\n",
    "        x = x.to(self.conv1.weight.dtype)\n",
    "        x = stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.attnpool(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Cell):\n",
    "    def construct(self, x: Tensor):\n",
    "        return x * ops.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "class ResidualAttentionBlock(nn.Cell):\n",
    "    def __init__(self, d_model: int, n_head: int, attn_mask: Tensor = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = nn.LayerNorm([d_model], epsilon=1e-5)\n",
    "        self.mlp = nn.SequentialCell(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"c_fc\", nn.Dense(d_model, d_model * 4)),\n",
    "                    (\"gelu\", QuickGELU()),\n",
    "                    (\"c_proj\", nn.Dense(d_model * 4, d_model)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        self.ln_2 = nn.LayerNorm([d_model], epsilon=1e-5)\n",
    "        # self.attn_mask = Parameter(attn_mask) if attn_mask is not None else None  TODO\n",
    "        if attn_mask is not None:\n",
    "            self.register_buffer(\"attn_mask\", attn_mask)\n",
    "        else:\n",
    "            self.register_buffer(\"attn_mask\", None)\n",
    "\n",
    "    def attention(self, x: Tensor):\n",
    "        if self.attn_mask is not None:\n",
    "            return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask.to(x.dtype))[0]\n",
    "        else:\n",
    "            return self.attn(x, x, x, need_weights=False)[0]\n",
    "\n",
    "    def construct(self, x: Tensor):\n",
    "        # x_type = x.dtype  TODO\n",
    "        # x = x + self.attention(self.ln_1(x.to(ms.float32)).to(x_type))\n",
    "        # x = x + self.mlp(self.ln_2(x.to(ms.float32)).to(x_type))\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Cell):\n",
    "    def __init__(self, width: int, layers: int, heads: int, attn_mask: Tensor = None):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.SequentialCell(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
    "\n",
    "    def construct(self, x: Tensor):\n",
    "        return self.resblocks(x)\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Cell):\n",
    "    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=width,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "            has_bias=False,\n",
    "            pad_mode=\"pad\",\n",
    "            weight_init=\"uniform\",\n",
    "            bias_init=\"uniform\",\n",
    "        )\n",
    "\n",
    "        scale = width**-0.5\n",
    "        self.class_embedding = Parameter(scale * ops.randn(width))\n",
    "        self.positional_embedding = Parameter(scale * ops.randn(((input_resolution // patch_size) ** 2 + 1, width)))\n",
    "        self.ln_pre = nn.LayerNorm([width], epsilon=1e-5)\n",
    "\n",
    "        self.transformer = Transformer(width, layers, heads)\n",
    "\n",
    "        self.ln_post = nn.LayerNorm([width], epsilon=1e-5)\n",
    "        self.proj = Parameter(scale * ops.randn((width, output_dim)))\n",
    "\n",
    "    def construct(self, x: Tensor):\n",
    "        x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "        x = x.reshape((x.shape[0], x.shape[1], -1))  # shape = [*, width, grid ** 2]\n",
    "        x = x.permute((0, 2, 1))  # shape = [*, grid ** 2, width]\n",
    "        x = ops.cat(\n",
    "            [self.class_embedding.to(x.dtype) + ops.zeros((x.shape[0], 1, x.shape[-1]), dtype=x.dtype), x], axis=1\n",
    "        )  # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        x_type = x.dtype\n",
    "        # x = self.ln_pre(x.to(ms.float32)).to(x_type)  TODO\n",
    "        x = self.ln_pre(x)\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "        x_type = x.dtype\n",
    "        # x = self.ln_post(x.to(ms.float32)[:, 0, :]).to(x_type)  TODO\n",
    "        x = self.ln_post(x[:, 0, :])\n",
    "\n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CLIP(nn.Cell):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        # vision\n",
    "        image_resolution: int,\n",
    "        vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "        vision_width: int,\n",
    "        vision_patch_size: int,\n",
    "        # text\n",
    "        context_length: int,\n",
    "        vocab_size: int,\n",
    "        transformer_width: int,\n",
    "        transformer_heads: int,\n",
    "        transformer_layers: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        if isinstance(vision_layers, (tuple, list)):\n",
    "            vision_heads = vision_width * 32 // 64\n",
    "            self.visual = ModifiedResNet(\n",
    "                layers=vision_layers,\n",
    "                output_dim=embed_dim,\n",
    "                heads=vision_heads,\n",
    "                input_resolution=image_resolution,\n",
    "                width=vision_width,\n",
    "            )\n",
    "        else:\n",
    "            vision_heads = vision_width // 64\n",
    "            self.visual = VisionTransformer(\n",
    "                input_resolution=image_resolution,\n",
    "                patch_size=vision_patch_size,\n",
    "                width=vision_width,\n",
    "                layers=vision_layers,\n",
    "                heads=vision_heads,\n",
    "                output_dim=embed_dim,\n",
    "            )\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask(),\n",
    "        )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "        self.positional_embedding = Parameter(ms.numpy.empty((self.context_length, transformer_width)))\n",
    "        self.ln_final = nn.LayerNorm([transformer_width], epsilon=1e-5)\n",
    "\n",
    "        self.text_projection = Parameter(ms.numpy.empty((transformer_width, embed_dim)))\n",
    "        self.logit_scale = Parameter(ops.ones((), ms.float32) * np.log(1 / 0.07))\n",
    "\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        self.token_embedding.embedding_table.set_data(\n",
    "            ops.normal(self.token_embedding.embedding_table.shape, stddev=0.02, mean=0)\n",
    "        )\n",
    "        self.positional_embedding.set_data(ops.normal(self.positional_embedding.shape, stddev=0.01, mean=0))\n",
    "\n",
    "        if isinstance(self.visual, ModifiedResNet):\n",
    "            if self.visual.attnpool is not None:\n",
    "                std = self.visual.attnpool.c_proj.in_channels**-0.5\n",
    "                self.visual.attnpool.q_proj.weight.set_data(\n",
    "                    ops.normal(self.visual.attnpool.q_proj.weight.shape, stddev=std, mean=0)\n",
    "                )\n",
    "                self.visual.attnpool.k_proj.weight.set_data(\n",
    "                    ops.normal(self.visual.attnpool.k_proj.weight.shape, stddev=std, mean=0)\n",
    "                )\n",
    "                self.visual.attnpool.v_proj.weight.set_data(\n",
    "                    ops.normal(self.visual.attnpool.v_proj.weight.shape, stddev=std, mean=0)\n",
    "                )\n",
    "                self.visual.attnpool.c_proj.weight.set_data(\n",
    "                    ops.normal(self.visual.attnpool.c_proj.weight.shape, stddev=std, mean=0)\n",
    "                )\n",
    "\n",
    "            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n",
    "                for param in resnet_block.get_parameters():\n",
    "                    if param.name.endswith(\"bn3.weight\"):\n",
    "                        param.set_data(ops.zeros(param.shape))\n",
    "\n",
    "        proj_std = (self.transformer.width**-0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self.transformer.width**-0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            block.attn.in_proj_weight.set_data(ops.normal(block.attn.in_proj_weight.shape, stddev=attn_std, mean=0))\n",
    "            block.attn.out_proj.weight.set_data(ops.normal(block.attn.out_proj.weight.shape, stddev=proj_std, mean=0))\n",
    "            block.mlp.c_fc.weight.set_data(ops.normal(block.mlp.c_fc.weight.shape, stddev=fc_std, mean=0))\n",
    "            block.mlp.c_proj.weight.set_data(ops.normal(block.mlp.c_proj.weight.shape, stddev=proj_std, mean=0))\n",
    "\n",
    "        if self.text_projection is not None:\n",
    "            self.text_projection.set_data(\n",
    "                ops.normal(self.text_projection.shape, stddev=self.transformer.width**-0.5, mean=0)\n",
    "            )\n",
    "\n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        mask = ops.fill(ms.float32, (self.context_length, self.context_length), float(\"-inf\"))\n",
    "        mask = mask.triu(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.conv1.weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.to(self.dtype))\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        x = self.token_embedding(text).to(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.to(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        # x = self.ln_final(x.to(ms.float32)).to(self.dtype)  TODO\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[ops.arange(x.shape[0]), text.argmax(axis=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "    def construct(self, image, text):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(text)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        # shape = [global_batch_size, global_batch_size]\n",
    "        return logits_per_image, logits_per_text\n",
    "\n",
    "\n",
    "def convert_weights(model: nn.Cell):\n",
    "    \"\"\"Convert applicable model parameters to fp16\"\"\"\n",
    "\n",
    "    def _convert_weights_to_fp16(layer):\n",
    "        if isinstance(layer, (nn.Conv1d, nn.Conv2d, nn.Dense)):\n",
    "            layer.weight.to(ms.float16)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.to(ms.float16)\n",
    "\n",
    "        if isinstance(layer, nn.MultiheadAttention):\n",
    "            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n",
    "                param = getattr(layer, attr)\n",
    "                if param is not None:\n",
    "                    param.to(ms.float16)\n",
    "\n",
    "        for name in [\"text_projection\", \"proj\"]:\n",
    "            if hasattr(layer, name):\n",
    "                attr = getattr(layer, name)\n",
    "                if attr is not None:\n",
    "                    attr.to(ms.float16)\n",
    "\n",
    "    model.apply(_convert_weights_to_fp16)\n",
    "\n",
    "\n",
    "def build_model(ckpt_dict: dict):\n",
    "    vit = \"visual.proj\" in ckpt_dict\n",
    "\n",
    "    if vit:\n",
    "        vision_width = ckpt_dict[\"visual.conv1.weight\"].shape[0]\n",
    "        vision_layers = len(\n",
    "            [k for k in ckpt_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")]\n",
    "        )\n",
    "        vision_patch_size = ckpt_dict[\"visual.conv1.weight\"].shape[-1]\n",
    "        grid_size = round((ckpt_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "        image_resolution = vision_patch_size * grid_size\n",
    "    else:\n",
    "        counts: list = [\n",
    "            len(set(k.split(\".\")[2] for k in ckpt_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]\n",
    "        ]\n",
    "        vision_layers = tuple(counts)\n",
    "        vision_width = ckpt_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n",
    "        output_width = round((ckpt_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "        vision_patch_size = None\n",
    "        assert output_width**2 + 1 == ckpt_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n",
    "        image_resolution = output_width * 32\n",
    "\n",
    "    embed_dim = ckpt_dict[\"text_projection\"].shape[1]\n",
    "    context_length = ckpt_dict[\"positional_embedding\"].shape[0]\n",
    "    vocab_size = ckpt_dict[\"token_embedding.embedding_table\"].shape[0]\n",
    "    transformer_width = ckpt_dict[\"ln_final.gamma\"].shape[0]\n",
    "    transformer_heads = transformer_width // 64\n",
    "    transformer_layers = len(set(k.split(\".\")[2] for k in ckpt_dict if k.startswith(\"transformer.resblocks\")))\n",
    "\n",
    "    model = CLIP(\n",
    "        embed_dim,\n",
    "        image_resolution,\n",
    "        vision_layers,\n",
    "        vision_width,\n",
    "        vision_patch_size,\n",
    "        context_length,\n",
    "        vocab_size,\n",
    "        transformer_width,\n",
    "        transformer_heads,\n",
    "        transformer_layers,\n",
    "    )\n",
    "\n",
    "    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
    "        if key in ckpt_dict:\n",
    "            del ckpt_dict[key]\n",
    "\n",
    "    convert_weights(model)\n",
    "    load_param_into_net(model, ckpt_dict)\n",
    "    return model.set_train(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import warnings\n",
    "from typing import List, Union\n",
    "from urllib import request\n",
    "\n",
    "from PIL import Image\n",
    "from pkg_resources import packaging\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor, load_checkpoint\n",
    "from mindspore.dataset.transforms import Compose\n",
    "from mindspore.dataset.vision import CenterCrop, Normalize, Resize, ToPIL, ToTensor\n",
    "\n",
    "\n",
    "try:\n",
    "    from mindspore.dataset.vision import Inter\n",
    "\n",
    "    BICUBIC = Inter.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "if packaging.version.parse(ms.__version__) < packaging.version.parse(\"2.0.0\"):\n",
    "    warnings.warn(\"MindSpore version 2.0.0 or higher is recommended\")\n",
    "\n",
    "__all__ = [\"available_models\", \"load\", \"tokenize\"]\n",
    "_tokenizer = SimpleTokenizer()\n",
    "\n",
    "_MODELS = {\n",
    "    \"RN50\": \"https://download.mindspore.cn/toolkits/mindcv/mindspore-clip/clip/RN50-5d39bdab.ckpt\",\n",
    "    \"RN101\": \"https://download.mindspore.cn/toolkits/mindcv/mindspore-clip/clip/RN101-a9edcaa9.ckpt\",\n",
    "    \"RN50x4\": \"https://download.mindspore.cn/toolkits/mindcv/mindspore-clip/clip/RN50x4-7b8cdb29.ckpt\",\n",
    "    \"RN50x16\": \"https://download.mindspore.cn/toolkits/mindcv/mindspore-clip/clip/RN50x16-66ea7861.ckpt\",\n",
    "    \"RN50x64\": \"https://download.mindspore.cn/toolkits/mindcv/mindspore-clip/clip/RN50x64-839951e0.ckpt\",\n",
    "    \"ViT-B/32\": \"https://download.mindspore.cn/toolkits/mindcv/mindspore-clip/clip/ViT_B_32-34c32b89.ckpt\",\n",
    "    \"ViT-B/16\": \"https://download.mindspore.cn/toolkits/mindcv/mindspore-clip/clip/ViT_B_16-99cbeeee.ckpt\",\n",
    "    \"ViT-L/14\": \"https://download.mindspore.cn/toolkits/mindcv/mindspore-clip/clip/ViT_L_14-1d8bde7f.ckpt\",\n",
    "    \"ViT-L/14@336px\": \"https://download.mindspore.cn/toolkits/mindcv/mindspore-clip/clip/ViT_L_14_336px-9ed46dee.ckpt\",\n",
    "}\n",
    "\n",
    "\n",
    "def _download(url: str, root: str):\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "    filename = os.path.basename(url)\n",
    "\n",
    "    expected_sha256 = url.split(\"/\")[-1].split(\"-\")[-1].split(\".\")[0]\n",
    "    download_target = os.path.join(root, filename)\n",
    "\n",
    "    if os.path.exists(download_target) and not os.path.isfile(download_target):\n",
    "        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n",
    "\n",
    "    if os.path.isfile(download_target):\n",
    "        if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() == expected_sha256:\n",
    "            return download_target\n",
    "        else:\n",
    "            warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
    "\n",
    "    with request.urlopen(url) as source, open(download_target, \"wb\") as output:\n",
    "        with tqdm(\n",
    "            total=int(source.info().get(\"Content-Length\")), ncols=80, unit=\"iB\", unit_scale=True, unit_divisor=1024\n",
    "        ) as loop:\n",
    "            while True:\n",
    "                buffer = source.read(8192)\n",
    "                if not buffer:\n",
    "                    break\n",
    "\n",
    "                output.write(buffer)\n",
    "                loop.update(len(buffer))\n",
    "\n",
    "    if expected_sha256 not in hashlib.sha256(open(download_target, \"rb\").read()).hexdigest():\n",
    "        raise RuntimeError(\"Model has been downloaded but the SHA256 checksum does not not match\")\n",
    "\n",
    "    return download_target\n",
    "\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "\n",
    "def _transform(n_px):\n",
    "    return Compose(\n",
    "        [\n",
    "            ToPIL(),\n",
    "            Resize(n_px, interpolation=BICUBIC),\n",
    "            CenterCrop(n_px),\n",
    "            _convert_image_to_rgb,\n",
    "            ToTensor(),\n",
    "            Normalize([0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711], is_hwc=False),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def available_models() -> List[str]:\n",
    "    \"\"\"Returns the names of available CLIP models\"\"\"\n",
    "    return list(_MODELS.keys())\n",
    "\n",
    "\n",
    "def load(name: str, device: str = \"Ascend\", mode: int = 1, download_root: str = None):\n",
    "    \"\"\"Load a CLIP model and a set of transform operations to the image input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        A model name or the path to a model checkpoint containing the parameter_dict, model names are listed by\n",
    "        `clip.available_models()`.\n",
    "\n",
    "    device : str\n",
    "        The device to put the loaded model, must be one of CPU, GPU, Ascend\n",
    "\n",
    "    mode : int\n",
    "        GRAPH_MODE(0) or PYNATIVE_MODE(1).\n",
    "\n",
    "    download_root: str\n",
    "        Path to download the model files; by default, it uses \"~/.cache/clip\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : mindspore.nn.Cell\n",
    "        The CLIP model\n",
    "\n",
    "    preprocess : Callable[[PIL.Image], mindspore.Tensor]\n",
    "        A mindspore vision transform that converts a PIL image into a tensor that the returned model can\n",
    "        take as its input.\n",
    "    \"\"\"\n",
    "    ms.set_context(device_target=device, mode=mode)\n",
    "    if mode == ms.GRAPH_MODE:\n",
    "        ms.set_context(jit_config={\"jit_level\": \"O2\"})\n",
    "    if name in _MODELS:\n",
    "        model_path = _download(_MODELS[name], download_root or os.path.expanduser(\"~/.cache/clip\"))\n",
    "        ckp_dict = load_checkpoint(model_path)\n",
    "    elif os.path.isfile(name):\n",
    "        ckp_dict = load_checkpoint(name)\n",
    "    else:\n",
    "        raise ValueError(f\"{name} not found; available models = {available_models()}\")\n",
    "\n",
    "    model = build_model(ckp_dict)\n",
    "    if str(device).lower() == \"cpu\":\n",
    "        model.to_float(ms.float32)\n",
    "    return model, _transform(model.visual.input_resolution)\n",
    "\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> Tensor:\n",
    "    \"\"\"\n",
    "    Returns the tokenized representation of given input string(s)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : Union[str, List[str]]\n",
    "        An input string or a list of input strings to tokenize\n",
    "\n",
    "    context_length : int\n",
    "        The context length to use; all CLIP models use 77 as the context length\n",
    "\n",
    "    truncate: bool\n",
    "        Whether to truncate the text in case its encoding is longer than the context length\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length].\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = ms.ops.zeros((len(all_tokens), context_length), dtype=ms.int32)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, : len(tokens)] = Tensor(tokens)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# TODO\n",
    "def tokenize_np(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the tokenized representation of given input string(s)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : Union[str, List[str]]\n",
    "        An input string or a list of input strings to tokenize\n",
    "\n",
    "    context_length : int\n",
    "        The context length to use; all CLIP models use 77 as the context length\n",
    "\n",
    "    truncate: bool\n",
    "        Whether to truncate the text in case its encoding is longer than the context length\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length].\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = np.zeros((len(all_tokens), context_length), dtype=np.int32)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, : len(tokens)] = tokens\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b58a3",
   "metadata": {},
   "source": [
    "# 数据集定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293bfdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.common import dtype as mstype\n",
    "import os\n",
    "from mindspore.dataset import Dataset\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "def get_cast_dtype(precision: str):\n",
    "    cast_dtype = None\n",
    "    if precision == 'bf16':\n",
    "        cast_dtype = mstype.bfloat16\n",
    "    elif precision == 'fp16':\n",
    "        cast_dtype = mstype.float16\n",
    "    else:\n",
    "        cast_dtype = mstype.float32\n",
    "    return cast_dtype\n",
    "\n",
    "def img_loader(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    if img.mode in (\"RGBA\", \"P\"):\n",
    "        img = img.convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "class DatasetFromFile(Dataset):\n",
    "    def __init__(self, filepath, label_filepath=None, transform=None, classes='asis'):\n",
    "        super(DatasetFromFile, self).__init__()\n",
    "        self.basefilepath = filepath\n",
    "        if label_filepath is None:\n",
    "            label_filepath = os.path.join(self.basefilepath, 'metadata.csv')\n",
    "        else:\n",
    "            label_filepath = os.path.join(self.basefilepath, label_filepath)\n",
    "\n",
    "        self.data = pd.read_csv(label_filepath, index_col=0).fillna('')\n",
    "        self.transform = transform\n",
    "        self.classes = self.data['class'].unique()\n",
    "        # create class_to_idx dict\n",
    "        if 'class_idx' in self.data.columns:\n",
    "            self.class_to_idx = dict([(x, y) for x, y in zip(self.data['class'], self.data['class_idx'])])\n",
    "        else:\n",
    "            self.class_to_idx = dict(zip(self.classes, range(len(self.classes))))\n",
    "            self.data['class_idx'] = self.data['class'].apply(lambda x: self.class_to_idx[x])\n",
    "\n",
    "        self.idx_to_class = dict([(v, k) for k, v in self.class_to_idx.items()])\n",
    "        self.samples = self.data['filepath'].values.tolist()\n",
    "\n",
    "        self.path_class_index = self.data.iloc\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._index >= len(self.data):\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            item = self.path_class_index[self._index]\n",
    "            filepath = os.path.join(self.basefilepath, item['filepath'].split('/')[-1])\n",
    "            img = img_loader(filepath)\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            label = item['class_idx']\n",
    "            output = (img, label)\n",
    "            self._index += 1\n",
    "            return output\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._index = 0\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4839f5a4",
   "metadata": {},
   "source": [
    "# 参数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import os\n",
    "\n",
    "class ParseKwargs(argparse.Action):\n",
    "    def __call__(self, parser, namespace, values, option_string=None):\n",
    "        kw = {}\n",
    "        for value in values:\n",
    "            key, value = value.split(\"=\")\n",
    "            try:\n",
    "                kw[key] = ast.literal_eval(value)\n",
    "            except ValueError:\n",
    "                # fallback to string (avoid need to escape on command line)\n",
    "                kw[key] = str(value)\n",
    "        setattr(namespace, self.dest, kw)\n",
    "\n",
    "\n",
    "def parse_args(args):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--label_filename\",\n",
    "        type=str,\n",
    "        default=\"metadata.csv\",\n",
    "        help=\"File path of the CSV annotation file under the base folder in --data_root.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--text_type\",\n",
    "        type=str,\n",
    "        default='asis',\n",
    "        help=\"Text type of annotations for test examples.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--task_type\",\n",
    "        type=str,\n",
    "        choices=[\"retrieve\", \"eval\", \"all\"],\n",
    "        default='all',\n",
    "        help=\"retrieve: retrieve image embeddings from image encoder; eval: evaluate with pickle file containing image embeddings; all: evaluate from scratch which both retrieve image embeddings and evaluate the embeddings in one run.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nfold\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "        help=\"The number of times of sampling training examples during few-shot.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--kshot_list\",\n",
    "        type=int,\n",
    "        nargs=\"+\",\n",
    "        default=[1,5],\n",
    "        help=\"A list of integers for k in k-shot.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_root\", \n",
    "        type=str, \n",
    "        default='/home/cti/hunan/project/python/bioclip-mindspore/data/plantnet_1k',\n",
    "        help=\"File path of base folder which contains images and a CSV annotation file. \"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logs\",\n",
    "        type=str,\n",
    "        default=\"/home/cti/hunan/project/python/bioclip-mindspore/log\",\n",
    "        help=\"Where to store logs. Use None or 'none' to avoid storing logs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Optional identifier for the experiment when storing logs. Otherwise use current time.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--workers\", type=int, default=1, help=\"Number of dataloader workers per GPU.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\", type=int, default=256, help=\"Batch size per GPU.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--precision\",\n",
    "        choices=[\"amp\", \"amp_bf16\", \"amp_bfloat16\", \"bf16\", \"fp32\"],\n",
    "        default=\"amp\",\n",
    "        help=\"Floating point precision.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        default=\"ViT-B-16\",\n",
    "        help=\"Name of the vision backbone to use.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pretrained\",\n",
    "        default=\"/home/cti/hunan/project/python/bioclip-mindspore/BIOCLIP.ckpt\",\n",
    "        type=str,\n",
    "        help=\"Use a pretrained CLIP model weights with the specified tag or file path. If running few-shot.py with --task_type=eval, use this parameter as the pickle file path.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pretrained-image\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Load imagenet pretrained weights for image tower backbone if available.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image-mean\",\n",
    "        type=float,\n",
    "        nargs=\"+\",\n",
    "        default=None,\n",
    "        metavar=\"MEAN\",\n",
    "        help=\"Override default image mean value of dataset\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image-std\",\n",
    "        type=float,\n",
    "        nargs=\"+\",\n",
    "        default=None,\n",
    "        metavar=\"STD\",\n",
    "        help=\"Override default image std deviation of of dataset\",\n",
    "    )\n",
    "    parser.add_argument(\"--aug-cfg\", nargs=\"*\", default={}, action=ParseKwargs)\n",
    "    parser.add_argument(\n",
    "        \"--force-image-size\",\n",
    "        type=int,\n",
    "        nargs=\"+\",\n",
    "        default=None,\n",
    "        help=\"Override default image size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--force-quick-gelu\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Force use of QuickGELU activation for non-OpenAI transformer models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--force-custom-text\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Force use of CustomTextCLIP model (separate text-tower).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--torchscript\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"torch.jit.script the model, also uses jit version of OpenAI models if pretrained=='openai'\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--trace\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"torch.jit.trace the model for inference / eval only\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--debug\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"If true, more information is logged.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no-set-device-rank\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Don't set device index from local rank (when CUDA_VISIBLE_DEVICES restricted to one per proc).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", \n",
    "        type=int, \n",
    "        default=0, \n",
    "        help=\"Default random seed.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gpu\",\n",
    "        type=int,\n",
    "        default=-1,\n",
    "        help=\"Default use gpu or not.\"\n",
    "    )\n",
    "    args, _ = parser.parse_known_args(args)\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602b7f4c",
   "metadata": {},
   "source": [
    "# 工具包定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dae1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import mindspore\n",
    "\n",
    "\n",
    "def save_json(path, obj):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f)\n",
    "\n",
    "\n",
    "def load_json(filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def random_seed(seed=42, rank=0):\n",
    "    mindspore.set_seed(seed + rank)\n",
    "    np.random.seed(seed + rank)\n",
    "    random.seed(seed + rank)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def init_device(args):\n",
    "    args.distributed = False\n",
    "    args.world_size = 1\n",
    "    args.rank = 0  # global rank\n",
    "    args.local_rank = 0\n",
    "\n",
    "    if args.gpu != -1:\n",
    "        device = \"0\"\n",
    "        mindspore.set_device(\"GPU\", 0)\n",
    "    else:\n",
    "        device = \"CPU\"\n",
    "        mindspore.set_device(\"CPU\")\n",
    "    args.device = device\n",
    "    device = mindspore.get_context(\"device_target\")\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0c9e5",
   "metadata": {},
   "source": [
    "# CLIP文本模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2afa77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_classnames = [\"tench\", \"goldfish\", \"great white shark\", \"tiger shark\", \"hammerhead shark\", \"electric ray\",\n",
    "                        \"stingray\", \"rooster\", \"hen\", \"ostrich\", \"brambling\", \"goldfinch\", \"house finch\", \"junco\",\n",
    "                        \"indigo bunting\", \"American robin\", \"bulbul\", \"jay\", \"magpie\", \"chickadee\", \"American dipper\",\n",
    "                        \"kite (bird of prey)\", \"bald eagle\", \"vulture\", \"great grey owl\", \"fire salamander\",\n",
    "                        \"smooth newt\", \"newt\", \"spotted salamander\", \"axolotl\", \"American bullfrog\", \"tree frog\",\n",
    "                        \"tailed frog\", \"loggerhead sea turtle\", \"leatherback sea turtle\", \"mud turtle\", \"terrapin\",\n",
    "                        \"box turtle\", \"banded gecko\", \"green iguana\", \"Carolina anole\",\n",
    "                        \"desert grassland whiptail lizard\", \"agama\", \"frilled-necked lizard\", \"alligator lizard\",\n",
    "                        \"Gila monster\", \"European green lizard\", \"chameleon\", \"Komodo dragon\", \"Nile crocodile\",\n",
    "                        \"American alligator\", \"triceratops\", \"worm snake\", \"ring-necked snake\",\n",
    "                        \"eastern hog-nosed snake\", \"smooth green snake\", \"kingsnake\", \"garter snake\", \"water snake\",\n",
    "                        \"vine snake\", \"night snake\", \"boa constrictor\", \"African rock python\", \"Indian cobra\",\n",
    "                        \"green mamba\", \"sea snake\", \"Saharan horned viper\", \"eastern diamondback rattlesnake\",\n",
    "                        \"sidewinder rattlesnake\", \"trilobite\", \"harvestman\", \"scorpion\", \"yellow garden spider\",\n",
    "                        \"barn spider\", \"European garden spider\", \"southern black widow\", \"tarantula\", \"wolf spider\",\n",
    "                        \"tick\", \"centipede\", \"black grouse\", \"ptarmigan\", \"ruffed grouse\", \"prairie grouse\", \"peafowl\",\n",
    "                        \"quail\", \"partridge\", \"african grey parrot\", \"macaw\", \"sulphur-crested cockatoo\", \"lorikeet\",\n",
    "                        \"coucal\", \"bee eater\", \"hornbill\", \"hummingbird\", \"jacamar\", \"toucan\", \"duck\",\n",
    "                        \"red-breasted merganser\", \"goose\", \"black swan\", \"tusker\", \"echidna\", \"platypus\", \"wallaby\",\n",
    "                        \"koala\", \"wombat\", \"jellyfish\", \"sea anemone\", \"brain coral\", \"flatworm\", \"nematode\", \"conch\",\n",
    "                        \"snail\", \"slug\", \"sea slug\", \"chiton\", \"chambered nautilus\", \"Dungeness crab\", \"rock crab\",\n",
    "                        \"fiddler crab\", \"red king crab\", \"American lobster\", \"spiny lobster\", \"crayfish\", \"hermit crab\",\n",
    "                        \"isopod\", \"white stork\", \"black stork\", \"spoonbill\", \"flamingo\", \"little blue heron\",\n",
    "                        \"great egret\", \"bittern bird\", \"crane bird\", \"limpkin\", \"common gallinule\", \"American coot\",\n",
    "                        \"bustard\", \"ruddy turnstone\", \"dunlin\", \"common redshank\", \"dowitcher\", \"oystercatcher\",\n",
    "                        \"pelican\", \"king penguin\", \"albatross\", \"grey whale\", \"killer whale\", \"dugong\", \"sea lion\",\n",
    "                        \"Chihuahua\", \"Japanese Chin\", \"Maltese\", \"Pekingese\", \"Shih Tzu\", \"King Charles Spaniel\",\n",
    "                        \"Papillon\", \"toy terrier\", \"Rhodesian Ridgeback\", \"Afghan Hound\", \"Basset Hound\", \"Beagle\",\n",
    "                        \"Bloodhound\", \"Bluetick Coonhound\", \"Black and Tan Coonhound\", \"Treeing Walker Coonhound\",\n",
    "                        \"English foxhound\", \"Redbone Coonhound\", \"borzoi\", \"Irish Wolfhound\", \"Italian Greyhound\",\n",
    "                        \"Whippet\", \"Ibizan Hound\", \"Norwegian Elkhound\", \"Otterhound\", \"Saluki\", \"Scottish Deerhound\",\n",
    "                        \"Weimaraner\", \"Staffordshire Bull Terrier\", \"American Staffordshire Terrier\",\n",
    "                        \"Bedlington Terrier\", \"Border Terrier\", \"Kerry Blue Terrier\", \"Irish Terrier\",\n",
    "                        \"Norfolk Terrier\", \"Norwich Terrier\", \"Yorkshire Terrier\", \"Wire Fox Terrier\",\n",
    "                        \"Lakeland Terrier\", \"Sealyham Terrier\", \"Airedale Terrier\", \"Cairn Terrier\",\n",
    "                        \"Australian Terrier\", \"Dandie Dinmont Terrier\", \"Boston Terrier\", \"Miniature Schnauzer\",\n",
    "                        \"Giant Schnauzer\", \"Standard Schnauzer\", \"Scottish Terrier\", \"Tibetan Terrier\",\n",
    "                        \"Australian Silky Terrier\", \"Soft-coated Wheaten Terrier\", \"West Highland White Terrier\",\n",
    "                        \"Lhasa Apso\", \"Flat-Coated Retriever\", \"Curly-coated Retriever\", \"Golden Retriever\",\n",
    "                        \"Labrador Retriever\", \"Chesapeake Bay Retriever\", \"German Shorthaired Pointer\", \"Vizsla\",\n",
    "                        \"English Setter\", \"Irish Setter\", \"Gordon Setter\", \"Brittany dog\", \"Clumber Spaniel\",\n",
    "                        \"English Springer Spaniel\", \"Welsh Springer Spaniel\", \"Cocker Spaniel\", \"Sussex Spaniel\",\n",
    "                        \"Irish Water Spaniel\", \"Kuvasz\", \"Schipperke\", \"Groenendael dog\", \"Malinois\", \"Briard\",\n",
    "                        \"Australian Kelpie\", \"Komondor\", \"Old English Sheepdog\", \"Shetland Sheepdog\", \"collie\",\n",
    "                        \"Border Collie\", \"Bouvier des Flandres dog\", \"Rottweiler\", \"German Shepherd Dog\", \"Dobermann\",\n",
    "                        \"Miniature Pinscher\", \"Greater Swiss Mountain Dog\", \"Bernese Mountain Dog\",\n",
    "                        \"Appenzeller Sennenhund\", \"Entlebucher Sennenhund\", \"Boxer\", \"Bullmastiff\", \"Tibetan Mastiff\",\n",
    "                        \"French Bulldog\", \"Great Dane\", \"St. Bernard\", \"husky\", \"Alaskan Malamute\", \"Siberian Husky\",\n",
    "                        \"Dalmatian\", \"Affenpinscher\", \"Basenji\", \"pug\", \"Leonberger\", \"Newfoundland dog\",\n",
    "                        \"Great Pyrenees dog\", \"Samoyed\", \"Pomeranian\", \"Chow Chow\", \"Keeshond\", \"brussels griffon\",\n",
    "                        \"Pembroke Welsh Corgi\", \"Cardigan Welsh Corgi\", \"Toy Poodle\", \"Miniature Poodle\",\n",
    "                        \"Standard Poodle\", \"Mexican hairless dog (xoloitzcuintli)\", \"grey wolf\", \"Alaskan tundra wolf\",\n",
    "                        \"red wolf or maned wolf\", \"coyote\", \"dingo\", \"dhole\", \"African wild dog\", \"hyena\", \"red fox\",\n",
    "                        \"kit fox\", \"Arctic fox\", \"grey fox\", \"tabby cat\", \"tiger cat\", \"Persian cat\", \"Siamese cat\",\n",
    "                        \"Egyptian Mau\", \"cougar\", \"lynx\", \"leopard\", \"snow leopard\", \"jaguar\", \"lion\", \"tiger\",\n",
    "                        \"cheetah\", \"brown bear\", \"American black bear\", \"polar bear\", \"sloth bear\", \"mongoose\",\n",
    "                        \"meerkat\", \"tiger beetle\", \"ladybug\", \"ground beetle\", \"longhorn beetle\", \"leaf beetle\",\n",
    "                        \"dung beetle\", \"rhinoceros beetle\", \"weevil\", \"fly\", \"bee\", \"ant\", \"grasshopper\",\n",
    "                        \"cricket insect\", \"stick insect\", \"cockroach\", \"praying mantis\", \"cicada\", \"leafhopper\",\n",
    "                        \"lacewing\", \"dragonfly\", \"damselfly\", \"red admiral butterfly\", \"ringlet butterfly\",\n",
    "                        \"monarch butterfly\", \"small white butterfly\", \"sulphur butterfly\", \"gossamer-winged butterfly\",\n",
    "                        \"starfish\", \"sea urchin\", \"sea cucumber\", \"cottontail rabbit\", \"hare\", \"Angora rabbit\",\n",
    "                        \"hamster\", \"porcupine\", \"fox squirrel\", \"marmot\", \"beaver\", \"guinea pig\", \"common sorrel horse\",\n",
    "                        \"zebra\", \"pig\", \"wild boar\", \"warthog\", \"hippopotamus\", \"ox\", \"water buffalo\", \"bison\",\n",
    "                        \"ram (adult male sheep)\", \"bighorn sheep\", \"Alpine ibex\", \"hartebeest\", \"impala (antelope)\",\n",
    "                        \"gazelle\", \"arabian camel\", \"llama\", \"weasel\", \"mink\", \"European polecat\",\n",
    "                        \"black-footed ferret\", \"otter\", \"skunk\", \"badger\", \"armadillo\", \"three-toed sloth\", \"orangutan\",\n",
    "                        \"gorilla\", \"chimpanzee\", \"gibbon\", \"siamang\", \"guenon\", \"patas monkey\", \"baboon\", \"macaque\",\n",
    "                        \"langur\", \"black-and-white colobus\", \"proboscis monkey\", \"marmoset\", \"white-headed capuchin\",\n",
    "                        \"howler monkey\", \"titi monkey\", \"Geoffroy's spider monkey\", \"common squirrel monkey\",\n",
    "                        \"ring-tailed lemur\", \"indri\", \"Asian elephant\", \"African bush elephant\", \"red panda\",\n",
    "                        \"giant panda\", \"snoek fish\", \"eel\", \"silver salmon\", \"rock beauty fish\", \"clownfish\",\n",
    "                        \"sturgeon\", \"gar fish\", \"lionfish\", \"pufferfish\", \"abacus\", \"abaya\", \"academic gown\",\n",
    "                        \"accordion\", \"acoustic guitar\", \"aircraft carrier\", \"airliner\", \"airship\", \"altar\", \"ambulance\",\n",
    "                        \"amphibious vehicle\", \"analog clock\", \"apiary\", \"apron\", \"trash can\", \"assault rifle\",\n",
    "                        \"backpack\", \"bakery\", \"balance beam\", \"balloon\", \"ballpoint pen\", \"Band-Aid\", \"banjo\",\n",
    "                        \"baluster / handrail\", \"barbell\", \"barber chair\", \"barbershop\", \"barn\", \"barometer\", \"barrel\",\n",
    "                        \"wheelbarrow\", \"baseball\", \"basketball\", \"bassinet\", \"bassoon\", \"swimming cap\", \"bath towel\",\n",
    "                        \"bathtub\", \"station wagon\", \"lighthouse\", \"beaker\", \"military hat (bearskin or shako)\",\n",
    "                        \"beer bottle\", \"beer glass\", \"bell tower\", \"baby bib\", \"tandem bicycle\", \"bikini\",\n",
    "                        \"ring binder\", \"binoculars\", \"birdhouse\", \"boathouse\", \"bobsleigh\", \"bolo tie\", \"poke bonnet\",\n",
    "                        \"bookcase\", \"bookstore\", \"bottle cap\", \"hunting bow\", \"bow tie\", \"brass memorial plaque\", \"bra\",\n",
    "                        \"breakwater\", \"breastplate\", \"broom\", \"bucket\", \"buckle\", \"bulletproof vest\",\n",
    "                        \"high-speed train\", \"butcher shop\", \"taxicab\", \"cauldron\", \"candle\", \"cannon\", \"canoe\",\n",
    "                        \"can opener\", \"cardigan\", \"car mirror\", \"carousel\", \"tool kit\", \"cardboard box / carton\",\n",
    "                        \"car wheel\", \"automated teller machine\", \"cassette\", \"cassette player\", \"castle\", \"catamaran\",\n",
    "                        \"CD player\", \"cello\", \"mobile phone\", \"chain\", \"chain-link fence\", \"chain mail\", \"chainsaw\",\n",
    "                        \"storage chest\", \"chiffonier\", \"bell or wind chime\", \"china cabinet\", \"Christmas stocking\",\n",
    "                        \"church\", \"movie theater\", \"cleaver\", \"cliff dwelling\", \"cloak\", \"clogs\", \"cocktail shaker\",\n",
    "                        \"coffee mug\", \"coffeemaker\", \"spiral or coil\", \"combination lock\", \"computer keyboard\",\n",
    "                        \"candy store\", \"container ship\", \"convertible\", \"corkscrew\", \"cornet\", \"cowboy boot\",\n",
    "                        \"cowboy hat\", \"cradle\", \"construction crane\", \"crash helmet\", \"crate\", \"infant bed\",\n",
    "                        \"Crock Pot\", \"croquet ball\", \"crutch\", \"cuirass\", \"dam\", \"desk\", \"desktop computer\",\n",
    "                        \"rotary dial telephone\", \"diaper\", \"digital clock\", \"digital watch\", \"dining table\",\n",
    "                        \"dishcloth\", \"dishwasher\", \"disc brake\", \"dock\", \"dog sled\", \"dome\", \"doormat\", \"drilling rig\",\n",
    "                        \"drum\", \"drumstick\", \"dumbbell\", \"Dutch oven\", \"electric fan\", \"electric guitar\",\n",
    "                        \"electric locomotive\", \"entertainment center\", \"envelope\", \"espresso machine\", \"face powder\",\n",
    "                        \"feather boa\", \"filing cabinet\", \"fireboat\", \"fire truck\", \"fire screen\", \"flagpole\", \"flute\",\n",
    "                        \"folding chair\", \"football helmet\", \"forklift\", \"fountain\", \"fountain pen\", \"four-poster bed\",\n",
    "                        \"freight car\", \"French horn\", \"frying pan\", \"fur coat\", \"garbage truck\",\n",
    "                        \"gas mask or respirator\", \"gas pump\", \"goblet\", \"go-kart\", \"golf ball\", \"golf cart\", \"gondola\",\n",
    "                        \"gong\", \"gown\", \"grand piano\", \"greenhouse\", \"radiator grille\", \"grocery store\", \"guillotine\",\n",
    "                        \"hair clip\", \"hair spray\", \"half-track\", \"hammer\", \"hamper\", \"hair dryer\", \"hand-held computer\",\n",
    "                        \"handkerchief\", \"hard disk drive\", \"harmonica\", \"harp\", \"combine harvester\", \"hatchet\",\n",
    "                        \"holster\", \"home theater\", \"honeycomb\", \"hook\", \"hoop skirt\", \"gymnastic horizontal bar\",\n",
    "                        \"horse-drawn vehicle\", \"hourglass\", \"iPod\", \"clothes iron\", \"carved pumpkin\", \"jeans\", \"jeep\",\n",
    "                        \"T-shirt\", \"jigsaw puzzle\", \"rickshaw\", \"joystick\", \"kimono\", \"knee pad\", \"knot\", \"lab coat\",\n",
    "                        \"ladle\", \"lampshade\", \"laptop computer\", \"lawn mower\", \"lens cap\", \"letter opener\", \"library\",\n",
    "                        \"lifeboat\", \"lighter\", \"limousine\", \"ocean liner\", \"lipstick\", \"slip-on shoe\", \"lotion\",\n",
    "                        \"music speaker\", \"loupe magnifying glass\", \"sawmill\", \"magnetic compass\", \"messenger bag\",\n",
    "                        \"mailbox\", \"tights\", \"one-piece bathing suit\", \"manhole cover\", \"maraca\", \"marimba\", \"mask\",\n",
    "                        \"matchstick\", \"maypole\", \"maze\", \"measuring cup\", \"medicine cabinet\", \"megalith\", \"microphone\",\n",
    "                        \"microwave oven\", \"military uniform\", \"milk can\", \"minibus\", \"miniskirt\", \"minivan\", \"missile\",\n",
    "                        \"mitten\", \"mixing bowl\", \"mobile home\", \"ford model t\", \"modem\", \"monastery\", \"monitor\",\n",
    "                        \"moped\", \"mortar and pestle\", \"graduation cap\", \"mosque\", \"mosquito net\", \"vespa\",\n",
    "                        \"mountain bike\", \"tent\", \"computer mouse\", \"mousetrap\", \"moving van\", \"muzzle\", \"metal nail\",\n",
    "                        \"neck brace\", \"necklace\", \"baby pacifier\", \"notebook computer\", \"obelisk\", \"oboe\", \"ocarina\",\n",
    "                        \"odometer\", \"oil filter\", \"pipe organ\", \"oscilloscope\", \"overskirt\", \"bullock cart\",\n",
    "                        \"oxygen mask\", \"product packet / packaging\", \"paddle\", \"paddle wheel\", \"padlock\", \"paintbrush\",\n",
    "                        \"pajamas\", \"palace\", \"pan flute\", \"paper towel\", \"parachute\", \"parallel bars\", \"park bench\",\n",
    "                        \"parking meter\", \"railroad car\", \"patio\", \"payphone\", \"pedestal\", \"pencil case\",\n",
    "                        \"pencil sharpener\", \"perfume\", \"Petri dish\", \"photocopier\", \"plectrum\", \"Pickelhaube\",\n",
    "                        \"picket fence\", \"pickup truck\", \"pier\", \"piggy bank\", \"pill bottle\", \"pillow\", \"ping-pong ball\",\n",
    "                        \"pinwheel\", \"pirate ship\", \"drink pitcher\", \"block plane\", \"planetarium\", \"plastic bag\",\n",
    "                        \"plate rack\", \"farm plow\", \"plunger\", \"Polaroid camera\", \"pole\", \"police van\", \"poncho\",\n",
    "                        \"pool table\", \"soda bottle\", \"plant pot\", \"potter's wheel\", \"power drill\", \"prayer rug\",\n",
    "                        \"printer\", \"prison\", \"missile\", \"projector\", \"hockey puck\", \"punching bag\", \"purse\", \"quill\",\n",
    "                        \"quilt\", \"race car\", \"racket\", \"radiator\", \"radio\", \"radio telescope\", \"rain barrel\",\n",
    "                        \"recreational vehicle\", \"fishing casting reel\", \"reflex camera\", \"refrigerator\",\n",
    "                        \"remote control\", \"restaurant\", \"revolver\", \"rifle\", \"rocking chair\", \"rotisserie\", \"eraser\",\n",
    "                        \"rugby ball\", \"ruler measuring stick\", \"sneaker\", \"safe\", \"safety pin\", \"salt shaker\", \"sandal\",\n",
    "                        \"sarong\", \"saxophone\", \"scabbard\", \"weighing scale\", \"school bus\", \"schooner\", \"scoreboard\",\n",
    "                        \"CRT monitor\", \"screw\", \"screwdriver\", \"seat belt\", \"sewing machine\", \"shield\", \"shoe store\",\n",
    "                        \"shoji screen / room divider\", \"shopping basket\", \"shopping cart\", \"shovel\", \"shower cap\",\n",
    "                        \"shower curtain\", \"ski\", \"balaclava ski mask\", \"sleeping bag\", \"slide rule\", \"sliding door\",\n",
    "                        \"slot machine\", \"snorkel\", \"snowmobile\", \"snowplow\", \"soap dispenser\", \"soccer ball\", \"sock\",\n",
    "                        \"solar thermal collector\", \"sombrero\", \"soup bowl\", \"keyboard space bar\", \"space heater\",\n",
    "                        \"space shuttle\", \"spatula\", \"motorboat\", \"spider web\", \"spindle\", \"sports car\", \"spotlight\",\n",
    "                        \"stage\", \"steam locomotive\", \"through arch bridge\", \"steel drum\", \"stethoscope\", \"scarf\",\n",
    "                        \"stone wall\", \"stopwatch\", \"stove\", \"strainer\", \"tram\", \"stretcher\", \"couch\", \"stupa\",\n",
    "                        \"submarine\", \"suit\", \"sundial\", \"sunglasses\", \"sunglasses\", \"sunscreen\", \"suspension bridge\",\n",
    "                        \"mop\", \"sweatshirt\", \"swim trunks / shorts\", \"swing\", \"electrical switch\", \"syringe\",\n",
    "                        \"table lamp\", \"tank\", \"tape player\", \"teapot\", \"teddy bear\", \"television\", \"tennis ball\",\n",
    "                        \"thatched roof\", \"front curtain\", \"thimble\", \"threshing machine\", \"throne\", \"tile roof\",\n",
    "                        \"toaster\", \"tobacco shop\", \"toilet seat\", \"torch\", \"totem pole\", \"tow truck\", \"toy store\",\n",
    "                        \"tractor\", \"semi-trailer truck\", \"tray\", \"trench coat\", \"tricycle\", \"trimaran\", \"tripod\",\n",
    "                        \"triumphal arch\", \"trolleybus\", \"trombone\", \"hot tub\", \"turnstile\", \"typewriter keyboard\",\n",
    "                        \"umbrella\", \"unicycle\", \"upright piano\", \"vacuum cleaner\", \"vase\", \"vaulted or arched ceiling\",\n",
    "                        \"velvet fabric\", \"vending machine\", \"vestment\", \"viaduct\", \"violin\", \"volleyball\",\n",
    "                        \"waffle iron\", \"wall clock\", \"wallet\", \"wardrobe\", \"military aircraft\", \"sink\",\n",
    "                        \"washing machine\", \"water bottle\", \"water jug\", \"water tower\", \"whiskey jug\", \"whistle\",\n",
    "                        \"hair wig\", \"window screen\", \"window shade\", \"Windsor tie\", \"wine bottle\", \"airplane wing\",\n",
    "                        \"wok\", \"wooden spoon\", \"wool\", \"split-rail fence\", \"shipwreck\", \"sailboat\", \"yurt\", \"website\",\n",
    "                        \"comic book\", \"crossword\", \"traffic or street sign\", \"traffic light\", \"dust jacket\", \"menu\",\n",
    "                        \"plate\", \"guacamole\", \"consomme\", \"hot pot\", \"trifle\", \"ice cream\", \"popsicle\", \"baguette\",\n",
    "                        \"bagel\", \"pretzel\", \"cheeseburger\", \"hot dog\", \"mashed potatoes\", \"cabbage\", \"broccoli\",\n",
    "                        \"cauliflower\", \"zucchini\", \"spaghetti squash\", \"acorn squash\", \"butternut squash\", \"cucumber\",\n",
    "                        \"artichoke\", \"bell pepper\", \"cardoon\", \"mushroom\", \"Granny Smith apple\", \"strawberry\", \"orange\",\n",
    "                        \"lemon\", \"fig\", \"pineapple\", \"banana\", \"jackfruit\", \"cherimoya (custard apple)\", \"pomegranate\",\n",
    "                        \"hay\", \"carbonara\", \"chocolate syrup\", \"dough\", \"meatloaf\", \"pizza\", \"pot pie\", \"burrito\",\n",
    "                        \"red wine\", \"espresso\", \"tea cup\", \"eggnog\", \"mountain\", \"bubble\", \"cliff\", \"coral reef\",\n",
    "                        \"geyser\", \"lakeshore\", \"promontory\", \"sandbar\", \"beach\", \"valley\", \"volcano\", \"baseball player\",\n",
    "                        \"bridegroom\", \"scuba diver\", \"rapeseed\", \"daisy\", \"yellow lady's slipper\", \"corn\", \"acorn\",\n",
    "                        \"rose hip\", \"horse chestnut seed\", \"coral fungus\", \"agaric\", \"gyromitra\", \"stinkhorn mushroom\",\n",
    "                        \"earth star fungus\", \"hen of the woods mushroom\", \"bolete\", \"corn cob\", \"toilet paper\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "openai_imagenet_template = [\n",
    "    lambda c: f'a photo of {c}.',\n",
    "    lambda c: f'a bad photo of a {c}.',\n",
    "    lambda c: f'a photo of many {c}.',\n",
    "    lambda c: f'a sculpture of a {c}.',\n",
    "    lambda c: f'a photo of the hard to see {c}.',\n",
    "    lambda c: f'a low resolution photo of the {c}.',\n",
    "    lambda c: f'a rendering of a {c}.',\n",
    "    lambda c: f'graffiti of a {c}.',\n",
    "    lambda c: f'a bad photo of the {c}.',\n",
    "    lambda c: f'a cropped photo of the {c}.',\n",
    "    lambda c: f'a tattoo of a {c}.',\n",
    "    lambda c: f'the embroidered {c}.',\n",
    "    lambda c: f'a photo of a hard to see {c}.',\n",
    "    lambda c: f'a bright photo of a {c}.',\n",
    "    lambda c: f'a photo of a clean {c}.',\n",
    "    lambda c: f'a photo of a dirty {c}.',\n",
    "    lambda c: f'a dark photo of the {c}.',\n",
    "    lambda c: f'a drawing of a {c}.',\n",
    "    lambda c: f'a photo of my {c}.',\n",
    "    lambda c: f'the plastic {c}.',\n",
    "    lambda c: f'a photo of the cool {c}.',\n",
    "    lambda c: f'a close-up photo of a {c}.',\n",
    "    lambda c: f'a black and white photo of the {c}.',\n",
    "    lambda c: f'a painting of the {c}.',\n",
    "    lambda c: f'a painting of a {c}.',\n",
    "    lambda c: f'a pixelated photo of the {c}.',\n",
    "    lambda c: f'a sculpture of the {c}.',\n",
    "    lambda c: f'a bright photo of the {c}.',\n",
    "    lambda c: f'a cropped photo of a {c}.',\n",
    "    lambda c: f'a plastic {c}.',\n",
    "    lambda c: f'a photo of the dirty {c}.',\n",
    "    lambda c: f'a jpeg corrupted photo of a {c}.',\n",
    "    lambda c: f'a blurry photo of the {c}.',\n",
    "    lambda c: f'a photo of the {c}.',\n",
    "    lambda c: f'a good photo of the {c}.',\n",
    "    lambda c: f'a rendering of the {c}.',\n",
    "    lambda c: f'a {c} in a video game.',\n",
    "    lambda c: f'a photo of one {c}.',\n",
    "    lambda c: f'a doodle of a {c}.',\n",
    "    lambda c: f'a close-up photo of the {c}.',\n",
    "    lambda c: f'a photo of a {c}.',\n",
    "    lambda c: f'the origami {c}.',\n",
    "    lambda c: f'the {c} in a video game.',\n",
    "    lambda c: f'a sketch of a {c}.',\n",
    "    lambda c: f'a doodle of the {c}.',\n",
    "    lambda c: f'a origami {c}.',\n",
    "    lambda c: f'a low resolution photo of a {c}.',\n",
    "    lambda c: f'the toy {c}.',\n",
    "    lambda c: f'a rendition of the {c}.',\n",
    "    lambda c: f'a photo of the clean {c}.',\n",
    "    lambda c: f'a photo of a large {c}.',\n",
    "    lambda c: f'a rendition of a {c}.',\n",
    "    lambda c: f'a photo of a nice {c}.',\n",
    "    lambda c: f'a photo of a weird {c}.',\n",
    "    lambda c: f'a blurry photo of a {c}.',\n",
    "    lambda c: f'a cartoon {c}.',\n",
    "    lambda c: f'art of a {c}.',\n",
    "    lambda c: f'a sketch of the {c}.',\n",
    "    lambda c: f'a embroidered {c}.',\n",
    "    lambda c: f'a pixelated photo of a {c}.',\n",
    "    lambda c: f'itap of the {c}.',\n",
    "    lambda c: f'a jpeg corrupted photo of the {c}.',\n",
    "    lambda c: f'a good photo of a {c}.',\n",
    "    lambda c: f'a plushie {c}.',\n",
    "    lambda c: f'a photo of the nice {c}.',\n",
    "    lambda c: f'a photo of the small {c}.',\n",
    "    lambda c: f'a photo of the weird {c}.',\n",
    "    lambda c: f'the cartoon {c}.',\n",
    "    lambda c: f'art of the {c}.',\n",
    "    lambda c: f'a drawing of the {c}.',\n",
    "    lambda c: f'a photo of the large {c}.',\n",
    "    lambda c: f'a black and white photo of a {c}.',\n",
    "    lambda c: f'the plushie {c}.',\n",
    "    lambda c: f'a dark photo of a {c}.',\n",
    "    lambda c: f'itap of a {c}.',\n",
    "    lambda c: f'graffiti of the {c}.',\n",
    "    lambda c: f'a toy {c}.',\n",
    "    lambda c: f'itap of my {c}.',\n",
    "    lambda c: f'a photo of a cool {c}.',\n",
    "    lambda c: f'a photo of a small {c}.',\n",
    "    lambda c: f'a tattoo of the {c}.',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a10927",
   "metadata": {},
   "source": [
    "# 日志定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "def setup_logging(log_file, level, include_host=False):\n",
    "    if include_host:\n",
    "        import socket\n",
    "        hostname = socket.gethostname()\n",
    "        formatter = logging.Formatter(\n",
    "            f'%(asctime)s |  {hostname} | %(levelname)s | %(message)s', datefmt='%Y-%m-%d,%H:%M:%S')\n",
    "    else:\n",
    "        formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s', datefmt='%Y-%m-%d,%H:%M:%S')\n",
    "\n",
    "    logging.root.setLevel(level)\n",
    "    loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "    for logger in loggers:\n",
    "        logger.setLevel(level)\n",
    "\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logging.root.addHandler(stream_handler)\n",
    "\n",
    "    if log_file:\n",
    "        file_handler = logging.FileHandler(filename=log_file)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logging.root.addHandler(file_handler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53617e3",
   "metadata": {},
   "source": [
    "# zero-shot评估\n",
    "在主函数开头设置`sys.argv`参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53afecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from mindspore import Tensor, nn\n",
    "import mindspore\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# model, preprocess = load(\"../../BIOCLIP.ckpt\", device=\"GPU\")\n",
    "#\n",
    "# image = Tensor(preprocess(Image.open(\"D:/Users/user\\PycharmProjects\\clip-mindspore\\data\\insects_mini_1k\\images/0a028522-0177-4087-86fa-9d7391f69b6a.jpg\")))\n",
    "# text = tokenize([\"a diagram\", \"Onoclea sensibilis\", \"a cat\", \"Blastodacna bicristatella\", \"Bluethroat\"])\n",
    "#\n",
    "# image_features = model.encode_image(image)\n",
    "# text_features = model.encode_text(text)\n",
    "#\n",
    "# logits_per_image, logits_per_text = model(image, text)\n",
    "# probs = nn.Softmax(axis=-1)(logits_per_image).numpy()\n",
    "#\n",
    "# print(\"Label probs:\", probs)\n",
    "\n",
    "\n",
    "def get_dataloader(dataset):\n",
    "    return mindspore.dataset.GeneratorDataset(\n",
    "        source=dataset,\n",
    "        column_names=[\"img\", \"label\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def zero_shot_classifier(model, classnames, templates, args):\n",
    "    zeroshot_weights = []\n",
    "    for classname in tqdm(classnames):\n",
    "        texts = [template(classname) for template in templates]  # format with class\n",
    "        texts = tokenize(texts)  # tokenize\n",
    "        class_embeddings = model.encode_text(texts)\n",
    "        class_embedding = mindspore.ops.mean(mindspore.ops.L2Normalize(axis=-1)(class_embeddings), axis=0)\n",
    "        class_embedding /= class_embedding.norm()\n",
    "        zeroshot_weights.append(class_embedding)\n",
    "    zeroshot_weights = mindspore.ops.stack(zeroshot_weights, axis=1)\n",
    "    return zeroshot_weights\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    pred = output.topk(max(topk), 1, True, True)[1].t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    return dict([\n",
    "        (k, float(correct[:k].reshape(-1).float().sum(0, keepdim=True).numpy()))\n",
    "        for k in topk\n",
    "    ])\n",
    "\n",
    "\n",
    "def run(model, classifier, dataloader, args):\n",
    "    # get_autocast会将model设置为自动混合精度\n",
    "    # model = get_autocast(args.precision, model)\n",
    "    cast_dtype = get_cast_dtype(args.precision)\n",
    "    n = 0.0\n",
    "    topk = dict()\n",
    "    for i in (1, min(len(dataloader.source.classes), 3), min(len(dataloader.source.classes), 5)):\n",
    "        topk[i] = 0.0\n",
    "    for images, target in tqdm(dataloader.batch(args.batch_size), unit_scale=args.batch_size):\n",
    "        # images.shape: torch.Size([batch_size, 3 rgb channels, image_height, image_width])\n",
    "        images = images.squeeze(axis=1)  # batch load need to squeeze dimension\n",
    "        if cast_dtype is not None:\n",
    "            images = images.to(dtype=cast_dtype)\n",
    "\n",
    "        # with autocast():\n",
    "        # predict\n",
    "        image_features = model.encode_image(images)\n",
    "        image_features = mindspore.ops.L2Normalize(axis=-1)(image_features)\n",
    "        # logits = 100.0 * image_features @ classifier\n",
    "        logits = model.logit_scale.exp() * image_features @ classifier\n",
    "\n",
    "        # measure accuracy\n",
    "        acc = accuracy(logits, target, topk=topk.keys())\n",
    "        for k, v in acc.items():\n",
    "            topk[k] += v\n",
    "        n += images.shape[0]\n",
    "\n",
    "    for k, v in acc.items():\n",
    "        topk[k] /= n\n",
    "    return topk\n",
    "\n",
    "\n",
    "def zero_shot_eval(model, data, args):\n",
    "    results = {}\n",
    "\n",
    "    logging.info(\"Starting zero-shot.\")\n",
    "\n",
    "    for split in data:\n",
    "        logging.info(\"Building zero-shot %s classifier.\", split)\n",
    "        classnames = data[split].source.classes\n",
    "\n",
    "        classifier = zero_shot_classifier(\n",
    "            model, classnames, openai_imagenet_template, args\n",
    "        )\n",
    "\n",
    "        topk = run(model, classifier, data[split], args)\n",
    "\n",
    "        for k, v in topk.items():\n",
    "            results[f\"{split}-top{k}\"] = v\n",
    "\n",
    "        logging.info(\"Finished zero-shot %s with total %d classes.\", split, len(data[split].source.classes))\n",
    "\n",
    "    logging.info(\"Finished zero-shot.\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.argv = [\"main.py\", \"--gpu\", \"0\", \"--data_root\", \"/path_to_data\", \"--logs\", \"/path_to_logs\", \"--pretrained\", \"/path_to_model_ckpt/BIOCLIP.ckpt\"]\n",
    "    args = parse_args(sys.argv[1:])\n",
    "\n",
    "    device = init_device(args)\n",
    "\n",
    "    args.save_logs = args.logs and args.logs.lower() != \"none\"\n",
    "\n",
    "    # get the name of the experiments\n",
    "    if args.save_logs and args.name is None:\n",
    "        # sanitize model name for filesystem/uri use\n",
    "        model_name_safe = args.model.replace(\"/\", \"-\")\n",
    "        date_str = datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "        args.name = \"-\".join(\n",
    "            [\n",
    "                date_str,\n",
    "                f\"model_{model_name_safe}\",\n",
    "                f\"b_{args.batch_size}\",\n",
    "                f\"j_{args.workers}\",\n",
    "                f\"p_{args.precision}\",\n",
    "                \"zero_shot\",\n",
    "            ]\n",
    "        )\n",
    "    if args.save_logs is None:\n",
    "        args.log_path = None\n",
    "    else:\n",
    "        log_base_path = os.path.join(args.logs, args.name)\n",
    "        args.log_path = None\n",
    "        os.makedirs(log_base_path, exist_ok=True)\n",
    "        log_filename = \"out.log\"\n",
    "        args.log_path = os.path.join(log_base_path, log_filename)\n",
    "\n",
    "    # Setup text logger\n",
    "    args.log_level = logging.DEBUG if args.debug else logging.INFO\n",
    "    setup_logging(args.log_path, args.log_level)\n",
    "\n",
    "    if (\n",
    "            isinstance(args.force_image_size, (tuple, list))\n",
    "            and len(args.force_image_size) == 1\n",
    "    ):\n",
    "        # arg is nargs, single (square) image size list -> int\n",
    "        args.force_image_size = args.force_image_size[0]\n",
    "\n",
    "    random_seed(args.seed, 0)\n",
    "    mindspore.set_deterministic(True)\n",
    "    model, preprocess = load(args.pretrained, device=device)\n",
    "\n",
    "    random_seed(args.seed, args.rank)\n",
    "\n",
    "    logging.info(\"Model:\")\n",
    "    logging.info(f\"{str(model)}\")\n",
    "    logging.info(\"Params:\")\n",
    "    if args.save_logs is None:\n",
    "        for name in sorted(vars(args)):\n",
    "            val = getattr(args, name)\n",
    "            logging.info(f\"  {name}: {val}\")\n",
    "    else:\n",
    "        params_file = os.path.join(args.logs, args.name, \"params.txt\")\n",
    "        with open(params_file, \"w\") as f:\n",
    "            for name in sorted(vars(args)):\n",
    "                val = getattr(args, name)\n",
    "                logging.info(f\"  {name}: {val}\")\n",
    "                f.write(f\"{name}: {val}\\n\")\n",
    "\n",
    "    # initialize datasets\n",
    "    dataset = DatasetFromFile(args.data_root, args.label_filename, transform=preprocess, classes=args.text_type)\n",
    "    data = {\n",
    "        \"val-unseen\": get_dataloader(\n",
    "            dataset\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    metrics = zero_shot_eval(model, data, args)\n",
    "\n",
    "    logging.info(\"Results:\")\n",
    "    for key, value in metrics.items():\n",
    "        logging.info(f\"  {key}: {value * 100:.2f}\")\n",
    "    logging.info(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
